#Resources (objects)

  #Structure (see hierarchy.txt):
    services (svc)
    deployment (deploy)
      replicaset (rs)
        pod (po)
          #container (docker, not a k8n resource)

  #RESOURCE TYPES and abbreviations: https://kubernetes.io/docs/reference/kubectl/overview/#resource-types
    apiservices
    certificatesigningrequests    csr
    clusters
    clusterrolebindings
    clusterroles
    componentstatuses             cs
    configmaps                    cm
    controllerrevisions
    cronjobs
    customresourcedefinition      crd
    daemonsets                    ds
    deployments                   deploy
    endpoints                     ep
    events                        ev
    horizontalpodautoscalers      hpa
    ingresses                     ing
    jobs
    limitranges                   limits
    namespaces                    ns
    networkpolicies               netpol
    nodes                         no
    persistentvolumeclaims        pvc
    persistentvolumes             pv
    poddisruptionbudget           pdb
    podpreset
    pods                          po
    podsecuritypolicies           psp
    podtemplates
    replicasets                   rs
    replicationcontrollers        rc      #old, use deployments instead
    resourcequotas                quota
    rolebindings
    roles
    secrets
    serviceaccounts               sa
    services                      svc
    statefulsets
    storageclasses
    Output                        opti


#----------------------------------k8n CLUSTER---------------------------------#
Kubernetes master has config files for the daemons:
  /etc/kubernetes/manifests/kube-controller-manager.manifest

Example arguments:
  https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/

  kube-controller-manager
    --horizontal-pod-autoscaler-upscale-delay 1m0s

#===================================CONCEPTS===================================#
#-----------------------------------AFFINITY-----------------------------------#
  #Affinity: Where to place or not place

  #affinity replaces nodeSelector

  requiredDuringSchedulingIgnoredDuringExecution  #enforced affinity
  preferredDuringSchedulingIgnoredDuringExecution #soft affinity

  #Types:
    podAffinity   podAntiAffinity
    nodeAffinity  nodeAntiAffinity #beta

  #Operators:
    In
    NotIn
    Exists
    DoesNotExist
    Gt
    Lt

  #Examples:
    #Make sure StatefulSet spread across nodes:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - {{ template "cassandra.name" . }}
            topologyKey: "kubernetes.io/hostname"


#------------------------------------LABELS------------------------------------#

  kubectl label nodes <node-name> <label-key>=<label-value>
  kubectl get nodes --show-labels

  #Built-in Labels:]
    kubernetes.io/hostname
    failure-domain.beta.kubernetes.io/zone
    failure-domain.beta.kubernetes.io/region
    beta.kubernetes.io/instance-type
    beta.kubernetes.io/os
    beta.kubernetes.io/arch

#-------------------------------TAINT/TOLERATION-------------------------------#
  #Source: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/

  #Taint: Disallow
  #Toleration: Exceptions to taints

  #Add Taint:
  kubectl taint nodes node1 key=value:NoSchedule

  #Remove Taint:
  kubectl taint nodes node1 key=value:NoSchedule-

  #Built-in Taints:
  node.kubernetes.io/not-ready                      #NodeCondition Ready = "False"
  node.kubernetes.io/unreachable                    #Unreachable from the node controller. NodeCondition Ready = "Unknown"
  node.kubernetes.io/out-of-disk                    #Node becomes out of disk.
  node.kubernetes.io/memory-pressure                #Node has memory pressure.
  node.kubernetes.io/disk-pressure                  #Node has disk pressure.
  node.kubernetes.io/network-unavailable            #Node's network is unavailable.
  node.kubernetes.io/unschedulable                  #Node is unschedulable.
  node.cloudprovider.kubernetes.io/uninitialized    #When the kubelet is started with "external" cloud provider,
                                                    #   this taint is set on a node to mark it as unusable.
                                                    #   After a controller from the cloud-controller-manager initializes this node,
                                                    #   the kubelet removes this taint.

  #Default taint NoSchedule on master node(s):
  kubectl taint node mymasternode node-role.kubernetes.io/master:NoSchedule-


#===================================COMMANDS===================================#

  kubectl [action] [resource] [name] [flagged args]
    #can use a "/" between [resource] and [name]
    #can pass [flagged args] as "--arg=value" or "--arg value"
    #can use abbreviated aliases for [resource], in singular or plural form
    #can pass multiple resources: `kubectl delete pod one two three`

  #Cheat Sheet: https://kubernetes.io/docs/reference/kubectl/cheatsheet/

  #ACTIONS:

    #create:
    kubectl create -f resource.yaml
      #service (can also be created in yaml):
      kubectl expose deployment my-deployment --type="LoadBalancer" --name="my-pod"

    #update:
    kubectl apply -f resource.yaml

    #attach:
    kubectl exec -it my-deployment-98789595c-z6r9w -- /bin/bash
    kubectl -n my-namespace exec -c http-request -it my-pod-dd9b6b858-2q9r2 -- /bin/sh
    # -n = namespace

    #logs from stdout:
    kubectl logs my-deployment-8474cdbc55-kc78n
    kubectl logs my-pod -c my-container

    #Large logs:
      # Display only the most recent 20 lines of output in pod nginx
      kubectl logs --tail=20 nginx

      # Show all logs from pod nginx written in the last hour
      kubectl logs --since=1h nginx
    #source: https://stackoverflow.com/questions/51835066/tailing-few-lines-from-huge-logs-of-kubectl-logs-f

    #delete:
    kubectl delete -f resource.yaml
    kubectl delete service my-service
    kubectl delete service -l key=value # -l selects groups of resources by labels
    kubectl delete $(kubectl get pvc -o=name) #delete all persistentvolumeclaims

    kubectl delete $(kubectl get pv -o=name)
    kubectl delete $(kubectl get crd -o=name)
    kubectl delete $(kubectl get sc -o=name)
    kubectl delete $(kubectl get pv -o jsonpath='{range.items[?(@.status.phase=="Released")]}{"pv/"}{.metadata.name}{"\n"}{end}') #delete released persistent volumes

    #set: update deployment image version:
    kubectl set image deployment/my-deployment my-pod=repo.intra.net/dev/my-pod:0.1

    #get:
    kubectl get deployments
    kubectl get pods
    kubectl get pod,statefulset #multiples
    kubectl get services
    kubectl get configmap ${name} -o=yaml
    kubectl get services my-service
    kubectl get services/my-service -o go-template='{{(index .spec.ports 0).nodePort}}')
    kubectl get all #alias for pod,service,deployment,replicaset,statefulset +others?
    kubectl get pod --all-namespaces

    kubectl get pod my-pod -o=jsonpath="{.spec.containers[*].name}"
    kubectl -n my-namespace get pod my-pod-dd9b6b858-2q9r2 -o=json | jq -r '.spec.containers[].name'
    kubectl get pv -o jsonpath='{range.items[?(@.status.phase=="Released")]}{.metadata.name}{"\n"}{end}' #get all persistent volumes with released claims

      -o --output             output format
        name                  print resource names
        wide                  node name included
        yaml                  yaml-formatted api object
        json                  json-formatted api object
        jsonpath              jsonpath expression of fields to print
        jsonpath-file         file with jsonpath expression of fields to print
        custom-columns        print table with col_name01,col_name02,...
        custom-columns-file   file with table column names
      -v --v                  verbosity 0-9

    #get k8s nodes:
    kubectl get nodes

    #get k8s system pods:
    kubectl get pods -n kube-system
    #if something isn't running you are expecting, try rebooting
    #the network plugin "weave" uses two pods per node

    #describe:
    kubectl describe deployments
    kubectl describe deployment my-deployment
    kubectl describe pods my-deployment
    kubectl describe pod my-deployment-98789595c-z6r9w

    #troubleshooting:
    kubectl get pod my-pod -o=yaml
    kubectl get pod --output=yaml
    #source: https://kubernetes.io/docs/tasks/debug-application-cluster/determine-reason-pod-failure/

    #help/documentation:
    kubectl explain hpa.spec.scaleTargetRef

    #history:
    kubectl rollout history deployment/my-deployment #only displays CHANGE-CAUSE if --record passed during creation/update
    kubectl rollout history deployment/my-deployment --revision=1 #show what was changed in REVISION 1      

    #status:
    kubectl rollout status deploy/my-deployment

    #Flush pod within Deployment by changing a variable:
    kubectl set env deployment/my-deployment "DEPLOY_DATE=$(date +'%m-%d-%y-%T')"
    kubectl apply -f deployment.yaml

  #Delete retained local PersistentVolume permanently:
    kubectl delete pv my-persistent-volume
    kubectl delete pvc my-persistent-volume-claim #claim must be deleted
    #source: https://kubernetes.io/docs/tasks/run-application/delete-stateful-set/

  #Contexts:

    #See cluster information for the current context:
    kubectl cluster-info

    #List contexts:
    kubectl config get-contexts

    #Switch contexts:
    kubectl config use-context dev

    #Set context properties:
    kubectl config set-context my --cluster=my-cluster --user=my-admin --namespace my-namespace
    kubectl config set-credentials my-admin --client-certificate=my-cert.crt --client-key=my-cert.key

    #Set property not addressed by api (inline data):
    kubectl config set users.my-admin.client-key-data "$(grep 'client-key-data: ' /tmp/admin.conf | sed 's/^[ ]*.*: //')"
    kubectl config set users.my-admin.client-certificate-data "$(grep 'client-certificate-data: ' /tmp/admin.conf | sed 's/^[ ]*.*: //')"

    #Unset config:
    kubectl config unset contexts.my-context.namespace

    #Will unset namespace in ~/.kube/config:
      contexts:
      - context:
          cluster: my-cluster
          namespace: my-namespace
          user: my-admin
        name: my-context

  #Access Services
    #source: https://kubernetes.io/docs/tasks/administer-cluster/access-cluster-services/

    #get builtin services:
    kubectl cluster-info
      Kubernetes master is running at https://10.24.1.1:6443
      coredns is running at https://10.24.1.1:6443/api/v1/namespaces/kube-system/services/coredns:dns/proxy
      kubernetes-dashboard is running at https://10.24.1.1:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy

    #port forward:
    kubectl port-forward svc/service 8080:80
    kubectl port-forward my-pod 8080:80
      # <localhost port>:<service port>

  #MISC:

    #resources:
    kubectl set resources deployment my-deployment -c=resource --limits=cpu=200m,memory=512Mi

    #pause:
    kubectl rollout pause deployment/my-deployment

    #revert:
    kubectl rollout undo deployment/my-deployment
    kubectl rollout undo deployment/my-deployment --to-revision=2

    #scale:
    kubectl scale deployment my-deployment --replicas=3
    kubectl scale deployment my-deployment --min=10 --max=15 --cpu-percent=80

    #label:
    kubectl label pod my-deployment-98789595c-z6r9w key=value

    #opens editor for editing yaml state used currently:
    kubectl edit deployment/my-deployment

    #patch:
    kubectl patch deployment my-deployment --patch "$(cat patch.yaml)"
    kubectl patch deploy/my-deployment -p '{"spec":{"progressDeadlineSeconds":600}}'

    #Run pods:
    kubectl run hello-world --replicas=5 --labels="run=load-balancer-example" --image=gcr.io/google-samples/node-hello:1.0  --port=8080

    #Run Centos 7 from our repo:
    docker run -it --rm -v ${HOME}:/root/ -h centos7 repo.intra.net/dev/centos:7 /bin/bash

    #Rolling updates (replaced by Deployments):
    kubectl rolling-update -f my-rolling-update.yaml
    kubectl replace -f my-rolling-update.yaml

    #get minicube IP:
    minikube ip